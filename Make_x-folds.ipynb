{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create one dataset per cross-validation fold. To do this, we combine all cage data from all cages except the test cage in one .hdf5 file and the test data in another .hdf5 file. We indicate which cage to leave out through the 'remove_index' variable. This again has to be run once for each fold...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import h5py\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select which fold to make (zero indexed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/path/to/your/.hdf5/data\"\n",
    "target_path = \"/path/to/your/.hdf5/targets\"\n",
    "\n",
    "# Index we don't want in this dataset\n",
    "remove_index = 5\n",
    "\n",
    "data_file_list = sorted(glob.glob(data_path))\n",
    "target_file_list = sorted(glob.glob(target_path))\n",
    "print(target_file_list)\n",
    "del data_file_list[remove_index]\n",
    "del target_file_list[remove_index]\n",
    "\n",
    "print(target_file_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create initial dataset and targets variables and fill them with the first cage not left out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = h5py.File(data_file_list[0], 'r')\n",
    "dataset = f['mydataset'].value\n",
    "print(dataset.shape)\n",
    "print(target_file_list[0])\n",
    "\n",
    "f = h5py.File(target_file_list[0], 'r')\n",
    "targets = f['mydataset'].value\n",
    "print(targets.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iteratively add all cages not left out to the dataset and targets variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for number, file in enumerate(data_file_list):\n",
    "    if number > 0:\n",
    "        f = h5py.File(file, 'r')\n",
    "        data = f['mydataset'].value\n",
    "        print(data.shape)\n",
    "        dataset = np.concatenate((dataset, data), axis=0)\n",
    "        \n",
    "for number, file in enumerate(target_file_list):\n",
    "    if number > 0:\n",
    "        f = h5py.File(file, 'r')\n",
    "        data = f['mydataset'].value\n",
    "        print(data.shape)\n",
    "        targets = np.concatenate((targets, data), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save x-fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data(dataset, save_path):\n",
    "    with h5py.File(save_path, \"w\") as f:\n",
    "            dset = f.create_dataset(\"mydataset\", data=dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = '/path/to/save/your/dataset/not_cage_'+str(remove_index+1)+'_all_dates_train'\n",
    "save_data(dataset, save_path+'_data.hdf5')\n",
    "save_data(targets, save_path+'_targets.hdf5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
